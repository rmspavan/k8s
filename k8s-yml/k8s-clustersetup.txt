******Ansible*****


hostnamectl set-hostname kubernetes-master
poweroff
clear
sudo dnf makecache
sudo dnf install epel-release && sudo dnf makecache
sudo dnf install ansible

****K8s-using playbook*****

cat /etc/hosts
192.168.2.1 kubernetes-master.learnitguide.net kubernetes-master
192.168.2.2 kubernetes-worker1.learnitguide.net kubernetes-worker1
192.168.2.3 kubernetes-worker2.learnitguide.net kubernetes-worker2


ansible-playbook settingup_kubernetes_cluster.yml

ansible-playbook join_kubernetes_workers_nodes.yml

1. Make your servers ready (one master node and multiple worker nodes).

2. Make an entry of your each hosts in /etc/hosts file for name resolution.

3. Make sure kubernetes master node and other worker nodes are reachable between each other.

4. Internet connection must be enabled in all nodes, required packages will be downloaded from kubernetes official yum repository.

5. Clone this repository into your master node.

git clone https://github.com/learnitguide/kubernetes-and-ansible.git

6. once it is cloned, get into the directory

cd kubernetes-and-ansible/centos

7.	There is a file "hosts" available in "centos" directory, Just make your entries of your all kubernetes nodes.

8.	Provide your server details in "env_variables" available in "centos" directory.

9.	Deploy the ssh key from master node to other nodes for password less authentication.

ssh-keygen

10.	Copy the public key to all nodes including your master node and make sure you are able to login into any nodes without password.

11.	Run "settingup_kubernetes_cluster.yml" playbook to setup all nodes and kubernetes master configuration.

ansible-playbook settingup_kubernetes_cluster.yml

12.	Run "join_kubernetes_workers_nodes.yml" playbook to join the worker nodes with kubernetes master node once "settingup_kubernetes_cluster.yml" playbook tasks are completed.

ansible-playbook join_kubernetes_workers_nodes.yml

13.	Verify the configuration from master node.

kubectl get nodes

kubeadm join 192.168.0.116:6443 --token 5tjay3.ymdnm58x6dzrleu0 --discovery-token-ca-cert-hash sha256:e5e5904aef562fc71f638975af43c35936bafc88ffc16f9d4886de5415098143

****K8s-manually:***********


hostnamectl set-hostname worker-node-1
vi /etc/hosts
ping master-node
sudo yum install -y yum-utils
ip addr
poweroff
yum install lsof git curl wget net-tools -y
hostnamectl set-hostname kube-master

vi /etc/fstab (commentout the swap)
swapoff -a

firewall-cmd --permanent --add-port=6443/tcp
firewall-cmd --permanent --add-port=10250/tcp
systemctl status firewalld
sudo firewall-cmd --reload


cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf 
net.bridge.bridge-nf-call-ip6tables = 1 
net.bridge.bridge-nf-call-iptables = 1 
EOF
sysctl --system


cat <<EOF | sudo tee /etc/yum.repos.d/kubernetes.repo 
[kubernetes] 
name=Kubernetes 
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-\$basearch 
enabled=1 
gpgcheck=1 
repo_gpgcheck=1 
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg 
exclude=kubelet kubeadm kubectl 
EOF


setenforce 0 

sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config

sestatus

yum install -y yum-utils device-mapper-persistent-data lvm2

yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo 
yum update -y && yum install containerd.io docker-ce docker-ce-cli


mkdir /etc/docker 
cat > /etc/docker/daemon.json <<EOF 
{ 
  "exec-opts": ["native.cgroupdriver=systemd"], 
  "log-driver": "json-file", 
  "log-opts": { 
    "max-size": "100m" 
  }, 
  "storage-driver": "overlay2", 
  "storage-opts": [ 
    "overlay2.override_kernel_check=true" 
  ] 
} 
EOF


mkdir -p /etc/systemd/system/docker.service.d 
systemctl daemon-reload 
systemctl restart docker 
systemctl enable docker


yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes 
systemctl enable --now kubelet

cat /etc/sysconfig/network-scripts/ifcfg-enp0s3


kubeadm init --pod-network-cidr 10.244.0.0/16 --apiserver-advertise-address=192.168.0.120

watch docker images



kubeadm join 192.168.0.123:6443 --token st94e6.61c5env8jwg123n9 \\", "\t--discovery-token-ca-cert-hash sha256:88e1595ece46440e70efc89aaad2ab42490f741f1a4d68b6d624f667357fd9cc
kubeadm join 192.168.0.123:6443 --token st94e6.61c5env8jwg123n9 --discovery-token-ca-cert-hash sha256:88e1595ece46440e70efc89aaad2ab42490f741f1a4d68b6d624f667357fd9cc





kubeadm join 192.168.0.120:6443 --token 77h1ak.dkb8iu8lvwb3btrg --discovery-token-ca-cert-hash sha256:fe300860ee55a41082aa068d874d876dcb3e596318d604baf898b16ce9ef7c92


mkdir -p $HOME/ .kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

## to create network
kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')"


yum install bash-completion 
echo "source <(kubectl completion bash)" >> ~/.bashrc

Add Role

kubectl label node <node name> node-role.kubernetes.io/<role name>=<key - (any name)>

kubectl label node kubernetes-worker1 node-role.kubernetes.io/worker1=worker1


******Install Minikube:**************

dnf update -y
curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64
chmod +x minikube
install minikube /usr/local/bin/
minikube start --driver=none
minikube status
kubectl cluster-info


******AWX install in K8s:****************

minikube start --addons=ingress --cpus=2 --cni=flannel --install-addons=true --kubernetes-version=stable --memory=6g
 {{{{
  minikube start --addons=ingress --cpus=2 --install-addons=true --kubernetes-version=stable --memory=6g

  kubectl get po -A
  
  kubectl apply -f https://raw.githubusercontent.com/ansible/awx-operator/0.11.0/deploy/awx-operator.yaml
    


 minikube status
kubectl cluster-info
kubectl get nodes

kubectl apply -f https://raw.githubusercontent.com/ansible/awx-operator/0.12.0/deploy/awx-operator.yaml

vi ansible-awx.yml
---
apiVersion: awx.ansible.com/v1beta1
kind: AWX
metadata:
  name: ansible-awx
spec:
  service_type: nodeport
  ingress_type: none
  hostname: ansible-awx.example.com
  

 kubectl apply -f ansible-awx.yml
 
 kubectl logs -f deployment/awx-operator
 
 kubectl get pods -l "app.kubernetes.io/managed-by=awx-operator"
 
 To access AWX portal outside of minikube cluster, we must configure the tunneling, run
 
 $ nohup minikube tunnel & [3] 35709
 
 kubectl get svc ansible-awx-service
 
 kubectl port-forward svc/ansible-awx-service --address 0.0.0.0 32483:80 &> /dev/null & [4] 46686
 
  kubectl get secret ansible-awx-admin-password -o jsonpath="{.data.password}" | base64 --decode
  
  
  
To check port numbers
  
sudo lsof -i -P -n | grep LISTEN
sudo netstat -tulpn | grep LISTEN
sudo ss -tulpn | grep LISTEN
sudo lsof -i:22 ## see a specific port such as 22 ##
sudo nmap -sTU -O IP-address-Here



Firewall rules:

sudo firewall-cmd  --get-zones

sudo firewall-cmd --get-default-zone

sudo firewall-cmd --get-active-zones

sudo firewall-cmd --list-all

firewall-cmd --get-services

sudo firewall-cmd --zone=work --list-all

How to Change the Zone of an Interface (sudo firewall-cmd --zone=home --change-interface=eth1)

sudo firewall-cmd --set-default-zone=work (Change the Default firewalld Zone)

firewall-cmd --reload

killall -HUP firewalld




kubeadm token generate

kubeadm token create <generated-token> --print-join-command --ttl=0



The basic steps to stop/remove all containers and images
List all the containers. docker ps -aq.
Stop all running containers. docker stop $(docker ps -aq)
Remove all containers. docker rm $(docker ps -aq)
Remove all images. docker rmi $(docker images -q)